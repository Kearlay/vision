{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ships Detection using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Descriptions\n",
    "\n",
    "The dataset consists of image chips extracted from Planet satellite imagery collected over the San Francisco Bay and San Pedro Bay areas of California. It includes 4000 80x80 RGB images labeled with either a \"ship\" or \"no-ship\" classification. Image chips were derived from PlanetScope full-frame visual scene products, which are orthorectified to a 3 meter pixel size.\n",
    "\n",
    "Provided is a zipped directory shipsnet.zip that contains the entire dataset as .png image chips. Each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png\n",
    "\n",
    "label: Valued 1 or 0, representing the \"ship\" class and \"no-ship\" class, respectively.\n",
    "scene id: The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.\n",
    "longitude_latitude: The longitude and latitude coordinates of the image center point, with values separated by a single underscore.\n",
    "The dataset is also distributed as a JSON formatted text file shipsnet.json. The loaded object contains data, label, scene_ids, and location lists.\n",
    "\n",
    "The pixel value data for each 80x80 RGB image is stored as a list of 19200 integers within the data list. The first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue. The image is stored in row-major order, so that the first 80 entries of the array are the red channel values of the first row of the image.\n",
    "\n",
    "The list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import os,cv2\n",
    "import sys, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "#sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#keras\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam,adadelta\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset\n",
    "\n",
    "This block of code reads images from different labeled  folders,\n",
    "resizes all images (rows and columns), converts it to the gray channel\n",
    "and appends it to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Path=os.getcwd() # current directory\n",
    "data_path=Path+'/Data' #two folders inside Data floder(ships and no ships)\n",
    "data_dir_list=os.listdir(data_path) #Two folders in this case\n",
    "img_data_list=[] #Appends images from different labeled folders to this list\n",
    "for dataset in data_dir_list:\n",
    "    if dataset=='.DS_Store':\n",
    "        pass\n",
    "    else:\n",
    "        img_list=os.listdir(data_path+'/'+ dataset)\n",
    "        print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "        for img in img_list:\n",
    "            input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "            input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "            input_img_resize=cv2.resize(input_img,(80,80))\n",
    "            img_data_list.append(input_img_resize)\n",
    "\n",
    "\n",
    "#Input parameters\n",
    "\n",
    "img_rows=80 #image dimesions~ hight\n",
    "img_cols=80 #image dimesions~ width\n",
    "num_channel=1 #image dimesions~ Gray image\n",
    "num_epoch=20 # one epoch == one complete cycle of forward and back-propogation\n",
    "num_classes=2 #number of output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocssing the data\n",
    "Converts list of images to arrays using numpy, casts its type to float(for computation) and normalizes the images values by dividing it with max (255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_data=np.array(img_data_list) #Converts images to list of arrays\n",
    "img_data=img_data.astype('float32') #converting data it to float\n",
    "img_data/=255 #normalization\n",
    "print (img_data.shape)\n",
    "\n",
    "\n",
    "'''As theano and tensorflow takes differnt input dimensions this block of\n",
    "converts it to appropriate dimensions\n",
    "e.g. (number of images, channels, rows, columns)'''\n",
    "\n",
    "if num_channel==1: #for one channel\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data= np.expand_dims(img_data, axis=1)\n",
    "        print (img_data.shape)\n",
    "    else:\n",
    "        img_data= np.expand_dims(img_data, axis=4)\n",
    "        print (img_data.shape)\n",
    "else: # for RGB\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data=np.rollaxis(image_data,3,1)\n",
    "        print(img_data.shape)\n",
    "num_of_samples=img_data.shape[0] # Total number of images\n",
    "\n",
    "\n",
    "#### Labels\n",
    "'''As we read data from different folders sequentially we\n",
    "can assing labels mannualy'''\n",
    "\n",
    "labels=np.ones((num_of_samples,),dtype='int64') #sets length based on total number of images\n",
    "labels[0:699]=1\n",
    "labels[700:2799]=0\n",
    "names=['ships','no_ships']\n",
    "Y = np_utils.to_categorical(labels, num_classes) #to_categorical converts lables to array of inategers (dummy variable)\n",
    "\n",
    "\n",
    "#### Shuffeling and spliting data into train, validation, test dataset\n",
    "\n",
    "\n",
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "input_shape=img_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Designing CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3,3,border_mode='same',input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# Compiling CNN model (80-20)\n",
    "#Adam optimizer default parameters\n",
    "#with 20 Epochs\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Viewing model_configuration\n",
    "model.summary()\n",
    "model.get_config()\n",
    "model.layers[0].get_config()\n",
    "model.layers[0].input_shape\n",
    "model.layers[0].output_shape\n",
    "model.layers[0].get_weights()\n",
    "np.shape(model.layers[0].get_weights()[0])\n",
    "model.layers[0].trainable\n",
    "\n",
    "\n",
    "# Model fit\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=16, nb_epoch=num_epoch, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting the loss and accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['acc']\n",
    "val_acc=hist.history['val_acc']\n",
    "xc=range(num_epoch)\n",
    "\n",
    "plt.figure(1,figsize=(7,5))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'])\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "\n",
    "plt.figure(2,figsize=(7,5))\n",
    "plt.plot(xc,train_acc)\n",
    "plt.plot(xc,val_acc)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('train_acc vs val_acc')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'],loc=4)\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# Predicting new image class\n",
    "\n",
    "test_image = X_test[0:1]\n",
    "print (test_image.shape)\n",
    "print(model.predict(test_image))\n",
    "print(model.predict_classes(test_image))\n",
    "print(y_test[0:1])\n",
    "\n",
    "\n",
    "#### Testing a new iamge\n",
    "#Input new image, processing it to an original input dimentsions\n",
    "test_image = cv2.imread('/Users/Pushkar/Downloads/ships-in-satellite-imagery/resize/1-2750.png')\n",
    "test_image=cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "test_image=cv2.resize(test_image,(80,80))\n",
    "test_image = np.array(test_image)\n",
    "test_image = test_image.astype('float32')\n",
    "test_image /= 255\n",
    "print (test_image.shape)\n",
    "\n",
    "\n",
    "# Dimensions based on theano/tensorflow\n",
    "\n",
    "if num_channel==1:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        test_image= np.expand_dims(test_image, axis=0)\n",
    "        test_image= np.expand_dims(test_image, axis=0)\n",
    "        print (test_image.shape)\n",
    "    else:\n",
    "        test_image= np.expand_dims(test_image, axis=3)\n",
    "        test_image= np.expand_dims(test_image, axis=0)\n",
    "        print (test_image.shape)\n",
    "else:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        test_image=np.rollaxis(test_image,2,0)\n",
    "        test_image= np.expand_dims(test_image, axis=0)\n",
    "        print (test_image.shape)\n",
    "    else:\n",
    "        test_image= np.expand_dims(test_image, axis=0)\n",
    "        print (test_image.shape)\n",
    "\n",
    "\n",
    "### Predicting CLass of input image (0-noship,1-ship)\n",
    "\n",
    "# Predicting the test image\n",
    "print((model.predict(test_image)))\n",
    "print(model.predict_classes(test_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the intermediate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featuremaps(model, layer_idx, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()],[model.layers[layer_idx].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "layer_num=4\n",
    "filter_num=0\n",
    "\n",
    "activations = get_featuremaps(model, int(layer_num),test_image)\n",
    "print (np.shape(activations))\n",
    "feature_maps = activations[0][0]\n",
    "print (np.shape(feature_maps))\n",
    "\n",
    "if K.image_dim_ordering()=='th':\n",
    "    feature_maps=np.rollaxis((np.rollaxis(feature_maps,2,0)),2,0)\n",
    "print (feature_maps.shape)\n",
    "\n",
    "fig=plt.figure(figsize=(16,16))\n",
    "plt.imshow(feature_maps[:,:,filter_num],cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "num_of_featuremaps=feature_maps.shape[2]\n",
    "fig=plt.figure(figsize=(16,16))\n",
    "plt.title(\"featuremaps-layer-{}\".format(layer_num))\n",
    "subplot_num=int(np.ceil(np.sqrt(num_of_featuremaps)))\n",
    "for i in range(int(num_of_featuremaps)):\n",
    "    ax = fig.add_subplot(subplot_num, subplot_num, i+1)\n",
    "    ax.imshow(feature_maps[:,:,i],cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plotting the confusion matrix to understand the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the confusion matrix\n",
    "\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print(y_pred)\n",
    "target_names = ['class 0(No_ships)', 'class 1(Ships)']\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix')\n",
    "                     title='Normalized confusion matrix')\n",
    "#plt.figure()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
